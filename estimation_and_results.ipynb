{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4f5daf80-58d0-4bdc-9773-93acb125776c",
      "metadata": {
        "id": "4f5daf80-58d0-4bdc-9773-93acb125776c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dc3700a2-7530-4c4c-aea8-b9b0f404b945",
      "metadata": {
        "id": "dc3700a2-7530-4c4c-aea8-b9b0f404b945"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data: pd.DataFrame, start_year, t_limit, t_col=\"T\"):\n",
        "    \"\"\"\n",
        "    Remove unneccesary rows due to missing values, get relavent data for trial\n",
        "    \"\"\"\n",
        "    y_col = 'y_' + str(start_year + 1)\n",
        "    data = data[(data['T_' + str(start_year)] > 0) & (data['T_' + str(start_year + 1)] > 0) & (data[y_col] > 0)]\n",
        "    \n",
        "    data = data.applymap(lambda x: -1 if x <= 0 else x)\n",
        "    data = data[(data[f\"{t_col}_{start_year}\"] < t_limit)]\n",
        "    data[f\"{t_col}_{start_year + 1}\"] = data[f\"{t_col}_{start_year + 1}\"].apply(lambda x: int(x >= t_limit))\n",
        "  \n",
        "    # in this part i'm removing unneccecary columns for analysis and converting categorial columns to str type\n",
        "    columns_to_remove_1 = [c for c in data.columns if c.startswith('y_') and not c.endswith(str(start_year + 1))]\n",
        "    columns_to_remove_2 = [c for c in data.columns if c.startswith('T_') and not c.endswith(str(start_year + 1))]\n",
        "    columns_to_remove_3 = ['PUBID']\n",
        "    columns_to_remove = columns_to_remove_1 + columns_to_remove_2 + columns_to_remove_3\n",
        "    columns_to_use = [c for c in data.columns if c not in columns_to_remove]\n",
        "    new_data = data[columns_to_use]\n",
        "    keep_as_is_columns_starters = ['income_', 'CVC_HOURS_WK_YR_ALL_', 'y_']\n",
        "    for c in columns_to_use:\n",
        "      cont_flag = False\n",
        "      for starter in keep_as_is_columns_starters:\n",
        "        if c.startswith(starter):\n",
        "          cont_flag = True\n",
        "      if not cont_flag:\n",
        "        new_data[c] = new_data[c].apply(str)\n",
        "\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "990dad3f-fa34-4e55-ac3c-061014f9b688",
      "metadata": {
        "id": "990dad3f-fa34-4e55-ac3c-061014f9b688"
      },
      "outputs": [],
      "source": [
        "def transform_data(data: pd.DataFrame, start_year, t_limit):\n",
        "    \"\"\"\n",
        "    Normalize values, calculate propensity, ...\n",
        "    :data: complete dataframe\n",
        "    :t_col: column of treatment\n",
        "    :y_col: columns of result\n",
        "    :t_limit: number at which we put the bound for having recieved treatment or not\n",
        "    \"\"\"\n",
        "    cols = [c for c in data.columns if not c.startswith('T_') and not c.startswith('y_')]\n",
        "    X = data[cols]\n",
        "    X = pd.get_dummies(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X)\n",
        "    X_normalized = scaler.transform(X)\n",
        "\n",
        "    t_col = 'T_' + str(start_year + 1)\n",
        "    t = data[t_col]\n",
        "\n",
        "    clf = LogisticRegression(random_state=0).fit(X_normalized, t)\n",
        "    prob = clf.predict_proba(X_normalized)[: , 1]\n",
        "    data['propensity'] = prob\n",
        "\n",
        "    data_treated = data[data[t_col] == '1']\n",
        "    data_control = data[data[t_col] == '0']\n",
        "    return data_control, data_treated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4b29200b-004d-4ccd-a4c7-fc2b7f9e6e81",
      "metadata": {
        "id": "4b29200b-004d-4ccd-a4c7-fc2b7f9e6e81"
      },
      "outputs": [],
      "source": [
        "def calc_IPW_ATT(data_0, data_1, start_year):\n",
        "    year = start_year + 1\n",
        "    IPW_substructed_upper = sum(data_0['y_' + str(year)] * (data_0['propensity'] / (1 - data_0['propensity'])))\n",
        "    IPW_substructed_lower = sum(data_0['propensity'] / (1 - data_0['propensity']))\n",
        "    IPW_ATT = data_1['y_' + str(year)].mean() - (IPW_substructed_upper / IPW_substructed_lower)\n",
        "    return IPW_ATT\n",
        "\n",
        "def calc_IPW_ATE(data_0, data_1, start_year):\n",
        "  year = start_year + 1\n",
        "  n = len(data_0) + len(data_1)\n",
        "  IPW_ATE = (1 / n) * (sum(data_1['y_' + str(year)] / data_1['propensity']) \n",
        "                     - sum(data_0['y_' + str(year)] / (1 - data_0['propensity'])))\n",
        "  return IPW_ATE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "def calc_ATT_Slearner(data, start_year):\n",
        "  year = start_year + 1\n",
        "  y_col = 'y_' + str(year)\n",
        "  t_col = 'T_' + str(start_year + 1)\n",
        "  cols = [c for c in data.columns if c not in {'propensity', y_col}]\n",
        "  x = data[cols]\n",
        "  x = pd.get_dummies(x)\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x)\n",
        "  x_normalized = scaler.transform(x)\n",
        "  y = data[y_col]\n",
        "  reg = LinearRegression().fit(x_normalized, y)\n",
        "\n",
        "  x_treated = x[x[t_col + '_' + str(1)] == 1] \n",
        "  x_treated[t_col + '_' + str(1)] = x_treated[t_col + '_' + str(1)].apply(lambda x: 0)\n",
        "  x_treated[t_col + '_' + str(0)] = x_treated[t_col + '_' + str(0)].apply(lambda x: 1)\n",
        "  x_treated_normalized = scaler.transform(x_treated)\n",
        "  y_pred = reg.predict(x_treated_normalized)\n",
        "\n",
        "  data_1 = data[data[t_col] == '1']\n",
        "  ATT = (1 / len(data_1)) * sum(data_1[y_col] - y_pred)\n",
        "  return ATT\n",
        "\n",
        "def calc_ATE_Slearner(data, start_year):\n",
        "  year = start_year + 1\n",
        "  y_col = 'y_' + str(year)\n",
        "  t_col = 'T_' + str(start_year + 1)\n",
        "  cols = [c for c in data.columns if c not in {'propensity', y_col}]\n",
        "  x = data[cols]\n",
        "  x = pd.get_dummies(x)\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x)\n",
        "  x_normalized = scaler.transform(x)\n",
        "  y = data[y_col]\n",
        "  reg = LinearRegression().fit(x_normalized, y)\n",
        "\n",
        "  x_treated = pd.get_dummies(x)\n",
        "  x_control = pd.get_dummies(x)\n",
        "\n",
        "  x_treated[t_col + '_' + str(1)] = x_treated[t_col + '_' + str(1)].apply(lambda x: 1)\n",
        "  x_treated[t_col + '_' + str(0)] = x_treated[t_col + '_' + str(0)].apply(lambda x: 0)\n",
        "\n",
        "  x_control[t_col + '_' + str(1)] = x_control[t_col + '_' + str(1)].apply(lambda x: 0)\n",
        "  x_control[t_col + '_' + str(0)] = x_control[t_col + '_' + str(0)].apply(lambda x: 1)\n",
        "\n",
        "  scaler.fit(x_treated)\n",
        "  scaler.fit(x_control)\n",
        "  \n",
        "  y_pred_treated = reg.predict(x_treated)\n",
        "  y_pred_control = reg.predict(x_control)\n",
        "\n",
        "  ATE = (1 / len(data)) * sum(y_pred_treated - y_pred_control)\n",
        "  return ATE\n",
        "    "
      ],
      "metadata": {
        "id": "wPbj9fmH-A-a"
      },
      "id": "wPbj9fmH-A-a",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_ATT_Tlearner(data, start_year):\n",
        "  y_col = 'y_' + str(start_year + 1)\n",
        "  t_col = 'T_' + str(start_year + 1)\n",
        "  data_0 = data[data[t_col] == '0']\n",
        "  cols = [c for c in data_0.columns if c not in {'propensity', y_col, t_col}]\n",
        "  \n",
        "  x_0 = data_0[cols]\n",
        "  for c in cols:\n",
        "    x_0[c] = x_0[c].astype(int)\n",
        "\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x_0)\n",
        "  x_0_normalized = scaler.transform(x_0)\n",
        "\n",
        "  y = data_0[y_col]\n",
        "\n",
        "  reg = LinearRegression().fit(x_0_normalized, y)\n",
        "\n",
        "  data_1 = data[data[t_col] == '1']\n",
        "  x_1 = data_1[cols]\n",
        "  for c in cols:\n",
        "    x_1[c] = x_1[c].astype(int)\n",
        "\n",
        "  x_1_normalized = scaler.transform(x_1)\n",
        "  y_pred = reg.predict(x_1_normalized)\n",
        "  y_true = data_1[y_col]\n",
        "\n",
        "  ATT = (1 / len(y_true)) * sum(y_true - y_pred)\n",
        "  return ATT\n",
        "\n",
        "def calc_ATE_Tlearner(data, start_year):\n",
        "  y_col = 'y_' + str(start_year + 1)\n",
        "  t_col = 'T_' + str(start_year + 1)\n",
        "  data_0 = data[data[t_col] == '0']\n",
        "  data_1 = data[data[t_col] == '1']\n",
        "  cols = [c for c in data_0.columns if c not in {'propensity', y_col, t_col}]\n",
        "\n",
        "  x = data[cols]\n",
        "  x_0 = data_0[cols]\n",
        "  x_1 = data_1[cols]\n",
        "\n",
        "  for c in cols:\n",
        "    x[c] = x[c].astype(int)\n",
        "    x_0[c] = x_0[c].astype(int)\n",
        "    x_1[c] = x_1[c].astype(int)\n",
        "  \n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(x)\n",
        "  x_normalized = scaler.transform(x)\n",
        "\n",
        "  scaler_0 = MinMaxScaler()\n",
        "  scaler_0.fit(x_0)\n",
        "  x_0_normalized = scaler_0.transform(x_0)\n",
        "\n",
        "  scaler_1 = MinMaxScaler()\n",
        "  scaler_1.fit(x_1)\n",
        "  x_1_normalized = scaler_1.transform(x_1)\n",
        "\n",
        "  y_0 = data_0[y_col]\n",
        "  reg_0 = LinearRegression().fit(x_0_normalized, y_0)\n",
        "\n",
        "  y_1 = data_1[y_col]\n",
        "  reg_1 = LinearRegression().fit(x_1_normalized, y_1)\n",
        "\n",
        "  y_1_pred = reg_1.predict(x_normalized)\n",
        "  y_0_pred = reg_0.predict(x_normalized)\n",
        "\n",
        "  ATE = (1 / len(data)) * sum(y_1_pred - y_0_pred)\n",
        "  return ATE"
      ],
      "metadata": {
        "id": "nW9XLqCwp08M"
      },
      "id": "nW9XLqCwp08M",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calc_ATT_matching(data, start_year, num_neighboors):\n",
        "  y_col = 'y_' + str(start_year + 1)\n",
        "  t_col = 'T_' + str(start_year + 1)\n",
        "  data_0 = data[data[t_col] == '0']\n",
        "  data_1 = data[data[t_col] == '1']\n",
        "\n",
        "  cols = cols = [c for c in data_0.columns if c not in {'propensity', y_col, t_col}]\n",
        "\n",
        "  x_0 = data_0[cols]\n",
        "  x_1 = data_1[cols]\n",
        "\n",
        "  for c in cols:\n",
        "    x_0[c] = x_0[c].astype(int)\n",
        "    x_1[c] = x_1[c].astype(int)\n",
        "\n",
        "  scaler_0 = MinMaxScaler()\n",
        "  scaler_0.fit(x_0)\n",
        "  x_0_normalized = scaler_0.transform(x_0)\n",
        "\n",
        "  scaler_1 = MinMaxScaler()\n",
        "  scaler_1.fit(x_1)\n",
        "  x_1_normalized = scaler_1.transform(x_1)\n",
        "\n",
        "  y_1 = data_1[y_col].to_numpy()\n",
        "  y_0 = data_0[y_col].to_numpy()\n",
        "\n",
        "    \n",
        "  nbrs = NearestNeighbors(n_neighbors=num_neighboors, algorithm='ball_tree').fit(x_0_normalized)\n",
        "  _, indices = nbrs.kneighbors(x_1_normalized)\n",
        "\n",
        "  ITT_list = []\n",
        "\n",
        "  for i, indices in enumerate(indices):\n",
        "    closest_y_0_avg = np.mean(y_0[indices])\n",
        "    ITT = y_1[i] - closest_y_0_avg\n",
        "    ITT_list.append(ITT)\n",
        "\n",
        "  ATT = np.mean(ITT_list)\n",
        "  return ATT\n",
        "\n",
        "def calc_ATE_matching(data, start_year, num_neighboors):\n",
        "  y_col = 'y_' + str(start_year + 1)\n",
        "  t_col = 'T_' + str(start_year + 1)\n",
        "  data_0 = data[data[t_col] == '0']\n",
        "  data_1 = data[data[t_col] == '1']\n",
        "\n",
        "  cols = cols = [c for c in data_0.columns if c not in {'propensity', y_col, t_col}]\n",
        "\n",
        "  x_0 = data_0[cols]\n",
        "  x_1 = data_1[cols]\n",
        "\n",
        "  for c in cols:\n",
        "    x_0[c] = x_0[c].astype(int)\n",
        "    x_1[c] = x_1[c].astype(int)\n",
        "\n",
        "  scaler_0 = MinMaxScaler()\n",
        "  scaler_0.fit(x_0)\n",
        "  x_0_normalized = scaler_0.transform(x_0)\n",
        "\n",
        "  scaler_1 = MinMaxScaler()\n",
        "  scaler_1.fit(x_1)\n",
        "  x_1_normalized = scaler_1.transform(x_1)\n",
        "\n",
        "  y_1 = data_1[y_col].to_numpy()\n",
        "  y_0 = data_0[y_col].to_numpy()\n",
        "\n",
        "    \n",
        "  nbrs_0 = NearestNeighbors(n_neighbors=num_neighboors, algorithm='ball_tree').fit(x_0_normalized)\n",
        "  _, indices_0 = nbrs_0.kneighbors(x_1_normalized)\n",
        "\n",
        "  nbrs_1 = NearestNeighbors(n_neighbors=num_neighboors, algorithm='ball_tree').fit(x_1_normalized)\n",
        "  _, indices_1 = nbrs_1.kneighbors(x_0_normalized)\n",
        "\n",
        "  ITE_0_list = []\n",
        "\n",
        "  for i, indices_0 in enumerate(indices_0):\n",
        "    closest_y_0_avg = np.mean(y_0[indices_0])\n",
        "    ITE = y_1[i] - closest_y_0_avg\n",
        "    ITE_0_list.append(ITE)\n",
        "\n",
        "  ITE_1_list = []\n",
        "\n",
        "  for i, indices_1 in enumerate(indices_1):\n",
        "    closest_y_1_avg = np.mean(y_1[indices_1])\n",
        "    ITE = closest_y_1_avg - y_0[i]\n",
        "    ITE_1_list.append(ITE)\n",
        "  \n",
        "  ATE = np.mean(ITE_0_list + ITE_1_list)\n",
        "  return ATE"
      ],
      "metadata": {
        "id": "Zk2LhcHPx402"
      },
      "id": "Zk2LhcHPx402",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_estimations(data, start_year, t_limit=6):\n",
        "  transformed_data_control, transformed_data_treated = transform_data(data, start_year, t_limit)\n",
        "\n",
        "  results = dict()\n",
        "  results['IPW'] = dict()\n",
        "  results['IPW']['ATT'] = calc_IPW_ATT(transformed_data_control, transformed_data_treated, start_year)\n",
        "  results['IPW']['ATE'] = calc_IPW_ATE(transformed_data_control, transformed_data_treated, start_year)\n",
        "\n",
        "  results['Slearner'] = dict()\n",
        "  results['Slearner']['ATT'] = calc_ATT_Slearner(data, start_year)\n",
        "  results['Slearner']['ATE'] = calc_ATE_Slearner(data, start_year)\n",
        "\n",
        "  results['Tlearner'] = dict()\n",
        "  results['Tlearner']['ATT'] = calc_ATT_Tlearner(data, start_year)\n",
        "  results['Tlearner']['ATE'] = calc_ATE_Tlearner(data, start_year)\n",
        "\n",
        "  results['matching'] = dict()\n",
        "  num_neighboors = 2\n",
        "  results['matching']['ATT'] = calc_ATT_matching(data, start_year, num_neighboors)\n",
        "  results['matching']['ATE'] = calc_ATE_matching(data, start_year, num_neighboors)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "jIfoxE2Qj6nv"
      },
      "id": "jIfoxE2Qj6nv",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('2009_2010_data.csv')\n",
        "data = preprocess_data(data, 2009, 6)\n",
        "results_2009_2010 = get_all_estimations(data, 2009)"
      ],
      "metadata": {
        "id": "hblPIqapn_yH"
      },
      "id": "hblPIqapn_yH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_2009_2010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyT2lQURxXC5",
        "outputId": "f2addbd0-e169-4a29-df9b-54ce6100dd75"
      },
      "id": "IyT2lQURxXC5",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'IPW': {'ATT': -0.06092193721630501, 'ATE': -0.18022658743835943},\n",
              " 'Slearner': {'ATT': -0.06831139976801309, 'ATE': -0.06623586994755692},\n",
              " 'Tlearner': {'ATT': -0.06035876006124587, 'ATE': -0.009077854482146397},\n",
              " 'matching': {'ATT': -0.06823144104803494, 'ATE': -0.0900486057815298}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('2010_2011_data.csv')\n",
        "data = preprocess_data(data, 2010, 6)\n",
        "results_2010_2011 = get_all_estimations(data, 2010)"
      ],
      "metadata": {
        "id": "tu7TPEPLpRzT"
      },
      "id": "tu7TPEPLpRzT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_2010_2011"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSWSxrymxZUD",
        "outputId": "16305557-ed4b-4ef7-b775-8cbc806d44d6"
      },
      "id": "FSWSxrymxZUD",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'IPW': {'ATT': -0.11082910278531433, 'ATE': -0.2850867922245144},\n",
              " 'Slearner': {'ATT': -0.11733213440011481, 'ATE': -0.1169025290716936},\n",
              " 'Tlearner': {'ATT': -0.08843769123012075, 'ATE': -0.08943324414182813},\n",
              " 'matching': {'ATT': -0.11882893226176808, 'ATE': -0.07103100980652001}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_confidence_intervals(data, start_year, B=1000, t_limit=6):\n",
        "  results = dict()\n",
        "  results['IPW'] = dict()\n",
        "  results['IPW']['ATT'] = []\n",
        "  results['IPW']['ATE'] = []\n",
        "  results['Slearner'] = dict()\n",
        "  results['Slearner']['ATT'] = []\n",
        "  results['Slearner']['ATE'] = []\n",
        "  results['Tlearner'] = dict()\n",
        "  results['Tlearner']['ATT'] = []\n",
        "  results['Tlearner']['ATE'] = []\n",
        "  results['matching'] = dict()\n",
        "  results['matching']['ATT'] = []\n",
        "  results['matching']['ATE'] = []\n",
        "\n",
        "  for b in range(B):\n",
        "    data_sample = data.sample(frac=1, replace=True)\n",
        "    res_sample = get_all_estimations(data_sample, start_year)\n",
        "    results['IPW']['ATT'].append(res_sample['IPW']['ATT'])\n",
        "    results['IPW']['ATE'].append(res_sample['IPW']['ATE'])\n",
        "    \n",
        "    results['Slearner']['ATT'].append(res_sample['Slearner']['ATT'])\n",
        "    results['Slearner']['ATE'].append(res_sample['Slearner']['ATE'])\n",
        "    \n",
        "    results['Tlearner']['ATT'].append(res_sample['Tlearner']['ATT'])\n",
        "    results['Tlearner']['ATE'].append(res_sample['Tlearner']['ATE'])\n",
        "\n",
        "    results['matching']['ATT'].append(res_sample['matching']['ATT'])\n",
        "    results['matching']['ATE'].append(res_sample['matching']['ATE'])\n",
        "  \n",
        "  results['IPW']['ATT'] = (sorted(results['IPW']['ATT'])[24], sorted(results['IPW']['ATT'])[974])\n",
        "  results['IPW']['ATE'] = (sorted(results['IPW']['ATE'])[24], sorted(results['IPW']['ATE'])[974])\n",
        "  \n",
        "  results['Slearner']['ATT'] = (sorted(results['Slearner']['ATT'])[24], sorted(results['Slearner']['ATT'])[974])\n",
        "  results['Slearner']['ATE'] = (sorted(results['Slearner']['ATE'])[24], sorted(results['Slearner']['ATE'])[974])\n",
        "  \n",
        "  results['Tlearner']['ATT'] = (sorted(results['Tlearner']['ATT'])[24], sorted(results['Tlearner']['ATT'])[974])\n",
        "  results['Tlearner']['ATE'] = (sorted(results['Tlearner']['ATE'])[24], sorted(results['Tlearner']['ATE'])[974])\n",
        "\n",
        "  results['matching']['ATT'] = (sorted(results['matching']['ATT'])[24], sorted(results['matching']['ATT'])[974])\n",
        "  results['matching']['ATE'] = (sorted(results['matching']['ATE'])[24], sorted(results['matching']['ATE'])[974])\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "PZets_dmdoFk"
      },
      "id": "PZets_dmdoFk",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('2009_2010_data.csv')\n",
        "data = preprocess_data(data, 2009, 6)\n",
        "results_ci_2009_2010 = get_confidence_intervals(data, 2009)"
      ],
      "metadata": {
        "id": "NsCrjs3AiKM7"
      },
      "id": "NsCrjs3AiKM7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_ci_2009_2010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT6UxPn5jw3e",
        "outputId": "a5f1e91a-e033-4c40-e307-a823ef691641"
      },
      "id": "TT6UxPn5jw3e",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'IPW': {'ATT': (-0.1735009009446955, 0.04402623662138527),\n",
              "  'ATE': (-0.4098587024412444, -0.03816965441676895)},\n",
              " 'Slearner': {'ATT': (-0.18309828814338233, 0.03224340458416477),\n",
              "  'ATE': (-0.18359275070350475, 0.03515625)},\n",
              " 'Tlearner': {'ATT': (-0.17151825233702947, 0.04572824837530061),\n",
              "  'ATE': (-0.22318333998498116, 0.18539760024446025)},\n",
              " 'matching': {'ATT': (-0.230605738575983, 0.09770742358078603),\n",
              "  'ATE': (-0.25326170376055257, 0.06280378613456127)}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('2010_2011_data.csv')\n",
        "data = preprocess_data(data, 2010, 6)\n",
        "results_ci_2010_2011 = get_confidence_intervals(data, 2010)"
      ],
      "metadata": {
        "id": "WqYXJSUcjt74"
      },
      "id": "WqYXJSUcjt74",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_ci_2010_2011"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYxzl_Sqj417",
        "outputId": "c1ff8576-15dc-4d34-887f-aa9e0c14c518"
      },
      "id": "ZYxzl_Sqj417",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'IPW': {'ATT': (-0.22122923039254871, -0.001689357403633629),\n",
              "  'ATE': (-0.5349458759734506, -0.07524969152284748)},\n",
              " 'Slearner': {'ATT': (-0.23375175561797754, -0.012759878615702479),\n",
              "  'ATE': (-0.23437499999999997, -0.013301749271137026)},\n",
              " 'Tlearner': {'ATT': (-0.19318979971096387, 0.012715347790432598),\n",
              "  'ATE': (-0.23488595833645406, 0.24767883315661018)},\n",
              " 'matching': {'ATT': (-0.29682179341657206, 0.034523809523809526),\n",
              "  'ATE': (-0.2280678505168301, 0.09978796713490591)}}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_before_and_after_sleep_difference_on_treated(path_to_data, start_year):\n",
        "  data = pd.read_csv(path_to_data)\n",
        "  data = data[(data['T_' + str(start_year)] < 6) & (data['T_' + str(start_year)] > 0)]\n",
        "  data['T_' + str(start_year + 1)] = data['T_' + str(start_year + 1)].apply(lambda x: x == 6)\n",
        "  data = data[(data['y_' + str(start_year)] > 0) & (data['y_' + str(start_year + 1)] > 0)]\n",
        "  data_1 = data[data['T_' + str(start_year + 1)] == 1]\n",
        "  return (data_1['y_' + str(start_year + 1)] - data_1['y_' + str(start_year)]).mean()"
      ],
      "metadata": {
        "id": "ed7oh1wBxjWO"
      },
      "id": "ed7oh1wBxjWO",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_before_and_after_sleep_difference_on_treated('2009_2010_data.csv', 2009)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fbEx-7Uc8Md",
        "outputId": "f51e0474-1ed8-4987-8e47-ec164e7c1dd2"
      },
      "id": "6fbEx-7Uc8Md",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.046153846153846156"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_before_and_after_sleep_difference_on_treated('2010_2011_data.csv', 2010)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAs_H6O5QGWh",
        "outputId": "6b1d4eff-e2c6-4074-b392-b27fb135ff8b"
      },
      "id": "fAs_H6O5QGWh",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.07958477508650519"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}